# serializer version: 1
# name: test_inference_batch_of_documents_cpu
  tuple(
    '[{"Document stem": "GEF.document.0.1", "Status": "\\u2713", "Exception": "N/A"}]',
    '''
      # Batch Inference Summary
      
      ## Overview
      - **Flow Run**: test-inference-batch-of-documents-cpu
      - **Classifier**: Q788:6vxrmcuf
      - **Total documents processed**: 1
      - **Successful documents**: 1
      - **Failed documents**: 0
      - **Unknown failures**: 0
  
    ''',
  )
# ---
# name: test_inference_batch_of_documents_cpu_empty_batch
  tuple(
    '[]',
    '''
      # Batch Inference Summary
      
      ## Overview
      - **Flow Run**: test-inference-batch-of-documents-cpu-empty-batch
      - **Classifier**: Q788:aaaa2222
      - **Total documents processed**: 0
      - **Successful documents**: 0
      - **Failed documents**: 0
      - **Unknown failures**: 0
  
    ''',
  )
# ---
# name: test_inference_batch_of_documents_cpu_with_failures
  tuple(
    '[{"Document stem": "NonExistent.doc.1", "Status": "\\u2717", "Exception": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."}, {"Document stem": "AnotherMissing.doc.2", "Status": "\\u2717", "Exception": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."}]',
    '''
      # Batch Inference Summary
      
      ## Overview
      - **Flow Run**: test-inference-batch-of-documents-cpu-with-failures
      - **Classifier**: Q788:aaaa2222
      - **Total documents processed**: 2
      - **Successful documents**: 0
      - **Failed documents**: 2
      - **Unknown failures**: 0
  
    ''',
  )
# ---
# name: test_run_classifier_inference_on_document
  list([
    LabelledPassage(id='2', text='Ministry of fishing, overfishing, seafood harvest and fisheries.'),
  ])
# ---
# name: test_store_inference_result
  dict({
    'successful_document_stems': list([
      'TEST.DOC.1.1',
    ]),
  })
# ---
# name: test_store_metadata
  Metadata(flow_run=FlowRun(id=UUID('0199bef8-7e41-7afc-9b4c-d3abd406be84'), name='test-flow-run', flow_id=UUID('b213352f-3214-48e3-8f5d-ec19959cb28e'), state_id=None, deployment_id=None, deployment_version=None, work_queue_name=None, flow_version=None, parameters={}, idempotency_key=None, context={}, empirical_policy=FlowRunPolicy(max_retries=0, retry_delay_seconds=0.0, retries=None, retry_delay=None, pause_keys=set(), resuming=False, retry_type=None), tags=['tag:value1', 'sha:abc123', 'branch:main'], labels={}, parent_task_run_id=None, run_count=0, expected_start_time=None, next_scheduled_start_time=None, start_time=None, end_time=None, total_run_time=datetime.timedelta(0), estimated_run_time=datetime.timedelta(0), estimated_start_time_delta=datetime.timedelta(0), auto_scheduled=False, infrastructure_document_id=None, infrastructure_pid=None, created_by=None, work_queue_id=None, work_pool_id=None, work_pool_name=None, state=Running(message=None, type=RUNNING, result=None), job_variables=None, state_type=None, state_name=None), run_output_identifier='2025-01-15T10:30-test-flow-run', classifier_specs=[ClassifierSpec(concept_id='xyz78abc', wikibase_id='Q788', classifier_id='abcd2345', classifiers_profile=None, wandb_registry_version=Version('v1'), compute_environment=None, dont_run_on=None)], config=Config(cache_bucket='test_bucket', aggregate_document_source_prefix='labelled_passages/', aggregate_inference_results_prefix='inference_results/', inference_document_source_prefix='embeddings_input/', inference_document_target_prefix='labelled_passages/', index_results_prefix='index_concepts/', bucket_region='eu-west-1', aws_env=<AwsEnv.sandbox: 'sandbox'>, pipeline_state_prefix='input/', local_classifier_dir=Path('data/processed/classifiers'), wandb_model_org='climatepolicyradar_UZODYJSN66HCQ', wandb_model_registry='test_org/test_wandb_model_registry', wandb_entity='test_entity', wandb_api_key=SecretStr('**********'), wikibase_username='test_username', wikibase_password=SecretStr('**********'), wikibase_url='https://test.test.test', s3_concurrency_limit=25, s3_read_timeout=300))
# ---
