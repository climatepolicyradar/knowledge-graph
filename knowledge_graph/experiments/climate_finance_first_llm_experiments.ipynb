{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate finance experiments\n",
    "The goal of this notebook is to find out (in a very rough first experiment kind of way) how well different model set-ups are able to pick up on the concept of a finance flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Two options here:\n",
    "\n",
    "1. Grab the file which is also used for our internal tools (vibe checker, argilla). This is a feather file that has already had a bunch of weighted sampling done to it to make it somewhat more representative of diverse perspectives. We assume this is stored in data/processed/passages_dataset.feather.\n",
    "2. Load in the whole open data repository. This takes a while to do and the lack of sampling probably makes it worse (plus, you might up with paragraphs that aren't in the vibe checker yet). I'm using option one here, but leaving in the code for two, as it doesn't require you to bother the data science team for a data file.\n",
    "\n",
    "Either way, for these experiments, we're only really interested in a small subset, so loading the whole dataset, only to then subset it to a few dozen is sort of overkill. So at the end of this, I'll write the subset to a little csv to use instead (as long as create_new_sample is set to True).\n",
    "\n",
    "If you want to re-use the code without changing the data **and** you have a justice_subset.csv already, just set reload_data to False.\n",
    "\n",
    "Finally, the subset is created based on some title keywords. For testing purposes, we might want to add a set of random docs. This is controlled with add_random_docs. False means don't add anything, an integer means add that nr of passages. A number between 0 and 1 will take a nr of random passages equal to that fraction of the length of the title-based subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_data = False\n",
    "data_source = \"feather\"  #'feather' or 'open_data'\n",
    "create_new_sample_csv = False\n",
    "add_random_docs = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not reload_data:\n",
    "    title_df = pd.read_csv(\"../data/processed/finance_subset.csv\", encoding=\"utf-8\")\n",
    "    df = title_df.copy()\n",
    "    print(\"Read from CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Feather file\n",
    "This is easy mode! First, let's load in the full dataset, then create a title-based subset and, if desired, add a random subset to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == \"feather\" and reload_data:\n",
    "    df = pd.read_feather(\"../data/processed/passages_dataset.feather\")\n",
    "    print(f\"Loaded feather file with {df.shape[0]} rows\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a title-based subset\n",
    "# Note that we're being a bit more lenient here than with the full open data\n",
    "# Because we're drawing from a smaller subset\n",
    "if data_source == \"feather\" and reload_data:\n",
    "    title_df = df[\n",
    "        (\n",
    "            df[\"document_metadata.document_title\"].str.contains(\n",
    "                \"financial\", case=False, na=False\n",
    "            )\n",
    "        )\n",
    "        | (\n",
    "            df[\"document_metadata.document_title\"].str.contains(\n",
    "                \"invest\", case=False, na=False\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # (df['document_metadata.document_title'].str.contains('monetary', case=False, na=False)) |\n",
    "        (\n",
    "            df[\"document_metadata.document_title\"].str.contains(\n",
    "                \"fund\", case=False, na=False\n",
    "            )\n",
    "        )\n",
    "        | (\n",
    "            df[\"document_metadata.document_title\"].str.contains(\n",
    "                \"budget\", case=False, na=False\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    print(f\"Titles with matching keywords: {title_df.shape}\")\n",
    "\n",
    "    # Seems we're getting some noise from carbon budget and one massive German law\n",
    "    title_df = title_df[\n",
    "        ~(\n",
    "            (\n",
    "                title_df[\"document_metadata.document_title\"].str.contains(\n",
    "                    \"carbon budget\", case=False, na=False\n",
    "                )\n",
    "            )\n",
    "            | (\n",
    "                title_df[\"document_metadata.document_title\"].str.contains(\n",
    "                    \"federal budget for the budget year\", case=False, na=False\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(title_df.shape)\n",
    "\n",
    "    # Filter for recent documents (convert to datetime first)\n",
    "    if \"document_metadata.publication_ts\" in df.columns:\n",
    "        df[\"document_metadata.publication_ts\"] = pd.to_datetime(\n",
    "            df[\"document_metadata.publication_ts\"], errors=\"coerce\"\n",
    "        )\n",
    "        title_df = title_df[\n",
    "            title_df[\"document_metadata.publication_ts\"] >= \"2019-01-01\"\n",
    "        ]\n",
    "        print(f\"Taking only recent: {title_df.shape}\")\n",
    "\n",
    "    # Filter out null values for key columns\n",
    "    title_df = title_df[\n",
    "        (title_df[\"document_metadata.import_id\"].notna())\n",
    "        & (title_df[\"document_metadata.source_url\"].notna())\n",
    "    ]\n",
    "\n",
    "    print(\"Created df based on title keywords\")\n",
    "    print(title_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze keyword contributions\n",
    "keywords = [\n",
    "    \"financ\",\n",
    "    \"financial\",\n",
    "    \"finances\",\n",
    "    \"financing\",\n",
    "    \"fund\",\n",
    "    \"funding\",\n",
    "    \"invest\",\n",
    "    \"investment\",\n",
    "    \"investing\",\n",
    "    \"monetary\",\n",
    "    \"budget\",\n",
    "    \"carbon budget\",\n",
    "]\n",
    "\n",
    "for keyword in keywords:\n",
    "    hits = (\n",
    "        df[\"document_metadata.document_title\"]\n",
    "        .str.contains(keyword, case=False, na=False)\n",
    "        .sum()\n",
    "    )\n",
    "    print(f\"'{keyword}': {hits:,} hits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random sample\n",
    "if data_source == \"feather\" and add_random_docs:\n",
    "    if add_random_docs == True:\n",
    "        N = int(0.5 * len(title_df))\n",
    "    elif add_random_docs < 1:\n",
    "        N = int(add_random_docs * len(title_df))\n",
    "    else:\n",
    "        N = int(add_random_docs)\n",
    "\n",
    "    # Sample random rows from the original df\n",
    "    sampled_passages = df.sample(n=N, random_state=42)\n",
    "\n",
    "    # Concat, drop duplicates, reset index\n",
    "    title_df = pd.concat([title_df, sampled_passages], ignore_index=True)\n",
    "    title_df = title_df.drop_duplicates(\n",
    "        subset=[\"text_block.index\", \"document_metadata.import_id\"]\n",
    "    )\n",
    "    title_df = title_df.reset_index(drop=True)\n",
    "\n",
    "    print(\"Added random passages -- new size:\")\n",
    "    print(title_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Open data\n",
    "This is harder. Loading the whole open data takes a few minutes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import tqdm as notebook_tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Using the public version of the repo for now\n",
    "REPO_NAME = \"ClimatePolicyRadar/all-document-text-data\"\n",
    "REPO_URL = f\"https://huggingface.co/datasets/{REPO_NAME}\"\n",
    "DATA_CACHE_DIR = \"../../cache\"\n",
    "\n",
    "REVISION = \"main\"  # Use this to set a commit hash. Recommended!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_data:\n",
    "    snapshot_download(\n",
    "        repo_id=REPO_NAME,\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=DATA_CACHE_DIR,\n",
    "        revision=REVISION,\n",
    "        allow_patterns=[\"*.parquet\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used if data_source is 'open_data' to speed things up as it is so large\n",
    "def create_db():\n",
    "    db = duckdb.connect(\"data.db\")  # Create a persistent database\n",
    "\n",
    "    # Authenticate (only needed if loading a private dataset)\n",
    "    # You'll need to log in using `huggingface-cli login` in your terminal first\n",
    "    # db.execute(\"CREATE SECRET hf_token (TYPE HUGGINGFACE, PROVIDER credential_chain);\")\n",
    "\n",
    "    # Drop the existing table if it exists (necessary if you want to update the fields)\n",
    "    db.execute(\"DROP TABLE IF EXISTS open_data\")\n",
    "\n",
    "    # Check if table exists\n",
    "    table_exists = (\n",
    "        db.execute(\n",
    "            \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'open_data'\"\n",
    "        ).fetchone()[0]\n",
    "        > 0\n",
    "    )\n",
    "\n",
    "    if not table_exists:\n",
    "        # Create a persistent table with only the columns we need\n",
    "        db.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE open_data AS \n",
    "            SELECT \n",
    "                \"document_metadata.geographies\",\n",
    "                \"document_metadata.corpus_type_name\",\n",
    "                \"document_metadata.publication_ts\",\n",
    "                \"document_metadata.import_id\",\n",
    "                \"document_metadata.translated\",\n",
    "                \"document_metadata.source_url\",\n",
    "                \"document_metadata.document_title\",\t\n",
    "                \"text_block.text\",\n",
    "                \"text_block.language\",\n",
    "                \"text_block.type\",\n",
    "                \"text_block.index\",\n",
    "                \"text_block.page_number\"\n",
    "            FROM read_parquet('{}/*.parquet')\n",
    "        \"\"\".format(DATA_CACHE_DIR)\n",
    "        )\n",
    "\n",
    "        # Create indexes for common query patterns\n",
    "        db.execute('CREATE INDEX idx_language ON open_data(\"text_block.language\")')\n",
    "        db.execute(\n",
    "            'CREATE INDEX idx_corpus_type ON open_data(\"document_metadata.corpus_type_name\")'\n",
    "        )\n",
    "        db.execute(\n",
    "            'CREATE INDEX idx_publication_ts ON open_data(\"document_metadata.publication_ts\")'\n",
    "        )\n",
    "        db.execute('CREATE INDEX idx_text_type ON open_data(\"text_block.type\")')\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of fairly recent documents based on title keywords\n",
    "if data_source == \"open_data\" and reload_data:\n",
    "    db = create_db()\n",
    "    title_df = db.sql(\n",
    "        \"\"\"\n",
    "    SELECT *\n",
    "    FROM open_data\n",
    "    WHERE (\n",
    "        LOWER(\"document_metadata.document_title\") LIKE '%finance%'\n",
    "        OR LOWER(\"document_metadata.document_title\") LIKE '%investment%'\n",
    "        OR LOWER(\"document_metadata.document_title\") LIKE '%financing%'\n",
    "    )\n",
    "    AND \"document_metadata.publication_ts\" >= '2020-01-01'\n",
    "    AND \"document_metadata.import_id\" IS NOT NULL\n",
    "    AND \"document_metadata.source_url\" IS NOT NULL\n",
    "        \"\"\"\n",
    "    ).to_df()\n",
    "    print(\"Created df based on title keywords\")\n",
    "    print(title_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == \"open_data\" and add_random_docs:\n",
    "    if add_random_docs == True:\n",
    "        N = int(0.5 * len(title_df))\n",
    "    if add_random_docs < 1:\n",
    "        N = int(add_random_docs * len(title_df))\n",
    "    else:\n",
    "        N = int(add_random_docs)\n",
    "\n",
    "    sampled_passages = db.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM open_data\n",
    "        USING SAMPLE reservoir({N} ROWS)\n",
    "        REPEATABLE(42)\n",
    "    \"\"\").df()\n",
    "\n",
    "    # Concat, drop duplicates, reset index\n",
    "    title_df = pd.concat([title_df, sampled_passages], ignore_index=True)\n",
    "    title_df.drop_duplicates(\n",
    "        subset=[\"text_block.index\", \"document_metadata.import_id\"], inplace=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(\"Added random passages -- new size:\")\n",
    "    print(title_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create csv\n",
    "Regardless of which method was chosen to get the data, we can save it to a csv (or load a prior one if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_new_sample_csv:\n",
    "    title_df.to_csv(\n",
    "        \"../data/processed/finance_subset.csv\", index=False, encoding=\"utf-8\"\n",
    "    )\n",
    "    df = title_df.copy()\n",
    "    print(\"Saved CSV\")\n",
    "\n",
    "# Let's shorten the column names\n",
    "df.columns = [col.split(\".\")[-1] for col in df.columns]\n",
    "\n",
    "print(f\"Total nr of paras: {df.shape[0]}\")\n",
    "print(f\"Total nr of documents: {len(df['document_title'].unique())}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Connecting to the LLMs and concept store\n",
    "The LLM classifier we import from src is built using pydantic. That gives us a lot of flexibility and [known models](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.KnownModelName) to choose from, as long as they are covered by our API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3  # AWS\n",
    "\n",
    "# Remember to login with aws sso login --profile labs\n",
    "session = boto3.Session(profile_name=\"labs\", region_name=\"eu-west-1\")\n",
    "ssm = session.client(\"ssm\")\n",
    "api_key = ssm.get_parameter(Name=\"OPENAI_API_KEY\", WithDecryption=True)[\"Parameter\"][\n",
    "    \"Value\"\n",
    "]\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "print(\"Connected with Open AI API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio  # Since we're working on notebookes, we need to handle the async bits\n",
    "\n",
    "from knowledge_graph.classifier.large_language_model import LLMClassifier\n",
    "from knowledge_graph.concept import Concept\n",
    "from knowledge_graph.wikibase import WikibaseSession\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "wikibase = WikibaseSession()\n",
    "async with WikibaseSession() as wikibase:\n",
    "    finance = await wikibase.get_concept_async(\"Q1343\")\n",
    "\n",
    "    print(finance)\n",
    "\n",
    "classifier = LLMClassifier(finance, model_name=\"gpt-4-turbo\")\n",
    "sentence = \"This money is directed towards climate mitiation\"\n",
    "\n",
    "classifier.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We are spending 1 million USD on climate action\"\n",
    "classifier.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We are spending 1 million USD on solar panels\"\n",
    "classifier.predict(sentence)\n",
    "# Nothing so model doesn't understand mitigation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LLMClassifier(finance, model_name=\"gpt-4o\")\n",
    "sentence = \"We are spending 1 million USD on solar panels\"\n",
    "\n",
    "classifier.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LLMClassifier(finance, model_name=\"gpt-4-turbo\")\n",
    "sentence = \"We are spending 1 million USD on solar panels\"\n",
    "\n",
    "classifier.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with WikibaseSession() as wikibase:\n",
    "    mitigation = await wikibase.get_concept_async(\"Q1344\")\n",
    "\n",
    "    print(mitigation)\n",
    "\n",
    "classifier = LLMClassifier(mitigation, model_name=\"gpt-4o\")\n",
    "sentence = \"We are spending 1 million USD on solar panels\"\n",
    "\n",
    "classifier.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tech works! But it is pretty discouraging that even a fairly expensive model can't get a simple example of climate finance... \n",
    "\n",
    "Let's explore the basic functionality\n",
    "-  By default, the LLM classifier takes all the info it can find from the concept and puts this in the prompt. But we can also create a custom concept. \n",
    "- Plus, we can run in batches also. Sentences without a predicted span return an empty list. Still a bit unclear to me if batch prediction with nest_asyncio might lead to problems, but it seems OK so far.\n",
    "- Let's also explore the default prompt and play around with that a tiny bit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom test concept\n",
    "new_concept = Concept(\n",
    "    preferred_label=\"Finance flow\",\n",
    "    alternative_labels=[\"flow of finance\", \"financial transfer\", \"monetary flow\"],\n",
    "    description=\"Money is either being spent or transferred between actors\",\n",
    "    definition=\"A movement of monetary value between entities over time\",\n",
    ")\n",
    "\n",
    "classifier = LLMClassifier(new_concept, model_name=\"gpt-4-turbo\")\n",
    "\n",
    "# batch predict\n",
    "sentences = [\n",
    "    \"This sentence is about climate finance flows\",\n",
    "    \"We are spending 100 million USD on solar panels\",\n",
    "    \"This sentence is about money but it's not going anywhere\",\n",
    "]\n",
    "[print(f\"\\n{c}\") for c in classifier.predict_batch(sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic finance flows might be an easier starting point, under the assumption that the bottleneck is climate knowledge of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind the difference between the template prompt and the fully filled out system prompt\n",
    "print(\"PROMPT TEMPLATE:\")\n",
    "print(classifier.system_prompt_template)\n",
    "print(\"\\n\\nSYSTEM PROMPT:\")\n",
    "print(classifier.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some concepts in a larger sample\n",
    "We'll create a smaller dataframe. Ones with only a short amount of text are unlikely to have enough information, so let's pick out slightly longer ones and only in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's figure out how long this really takes\n",
    "import time\n",
    "\n",
    "df.dropna(subset=\"text\", inplace=True)\n",
    "df = df[df[\"language\"] == \"en\"]\n",
    "df = df[df[\"text\"].str.len() > 60]\n",
    "small_df = df.sample(n=250, random_state=42)\n",
    "[print(f\"{row['text']}\\n\") for i, row in small_df.head().iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom test concept\n",
    "new_concept = Concept(\n",
    "    preferred_label=\"Finance flow\",\n",
    "    alternative_labels=[\n",
    "        \"flow of finance\",\n",
    "        \"financial transfer\",\n",
    "        \"establish fund\",\n",
    "        \"monetary flow\",\n",
    "        \"investing USD\",\n",
    "        \"loan to\",\n",
    "        \"subsidy totalling\",\n",
    "        \"financial flow\",\n",
    "        \"payment to\",\n",
    "        \"paying taxes\",\n",
    "        \"spending euros on\",\n",
    "        \"raising money for\",\n",
    "        \"add to the financial budget\",\n",
    "    ],\n",
    "    description=\"Something of monetary value is either being spent or transferred between actors.\",\n",
    "    definition=\"\"\"\n",
    "    A finance flow is an economic flow that reflects the creation, transformation, exchange, transfer, or extinction of economic value and involves changes in ownership of goods and/or financial assets, the provision of services, or the provision of labor and capital.\n",
    "    Leave out generic descriptions or financial actors. Include only statements where it is clear that actual money or something of monetary value is being exchanged, or will be exchanged in the future.\n",
    "    Only label the flow itself (i.e. how much is being spent on what).\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "wikibase = WikibaseSession()\n",
    "\n",
    "classifier = LLMClassifier(new_concept, model_name=\"gpt-4o\")\n",
    "\n",
    "print(\"Starting prediction\")\n",
    "t0 = time.time()\n",
    "prediction_spans = classifier.predict_batch(small_df[\"text\"].astype(str))\n",
    "tseconds = time.time() - t0\n",
    "print(f\"Finished in {tseconds:.2f} seconds\")\n",
    "print(f\"That's {tseconds / len(small_df):.3f} s per passage\\n\\nPositive examples:\")\n",
    "\n",
    "for prediction in prediction_spans:\n",
    "    if len(prediction) > 0:\n",
    "        for p in prediction:\n",
    "            print(p.text)\n",
    "            print(f\"=> {p.labelled_text}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this (and a few similar runs) we conclude:\n",
    "- this is an OK starting point\n",
    "- 4o mini seems good enough to get consistent results. Less luck with 4 nano and 3.5. \n",
    "- Giving enough examples that closely match actual language in texts is useful\n",
    "- Even so, the model is not super strict (which we might want it to be). In particular, it still includes potential finance or the *need* for finance.\n",
    "- the model is better at sentence classification; not great at pulling out the actual flow in the sentence (e.g. GW instead of the power purchase agreements), though it's not terrible at it either. \n",
    "- 4o is the exception here, getting OK at actually classifying the relevant part of the passage, especially with an additional nudge in the concept description (\"Only label the flow itself (i.e. how much is being spent on what\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare a few different prompts and models to each other more formally.\n",
    "async with WikibaseSession() as wikibase:\n",
    "    finance = await wikibase.get_concept_async(\"Q1343\")\n",
    "\n",
    "setups = {\n",
    "    # \"4o-mini\": LLMClassifier(new_concept,\n",
    "    #                         model_name='gpt-4o-mini',\n",
    "    #                         system_prompt_template=\n",
    "    #                         classifier.system_prompt_template  # Current template\n",
    "    # ),\n",
    "    # \"4turbo\": LLMClassifier(new_concept,\n",
    "    #                     model_name='gpt-4-turbo',\n",
    "    #                     system_prompt_template=\n",
    "    #                     classifier.system_prompt_template  # Current template\n",
    "    # ),\n",
    "    # \"concept_store_4o\": LLMClassifier(finance,\n",
    "    #                         model_name = 'gpt-4o',\n",
    "    #                         system_prompt_template=\n",
    "    #                         classifier.system_prompt_template\n",
    "    # ),\n",
    "    # \"4o\":LLMClassifier(new_concept,\n",
    "    #                        model_name='gpt-4o',\n",
    "    #                        system_prompt_template= classifier.system_prompt_template\n",
    "    # ),\n",
    "    \"best_effort\": LLMClassifier(\n",
    "        new_concept,\n",
    "        model_name=\"gpt-4o\",\n",
    "        system_prompt_template=\"\"\"\n",
    "    You are a specialist analyst, tasked with identifying mentions of \n",
    "    concepts in policy documents. You will mark up references to concepts with \n",
    "    XML tags.\n",
    "\n",
    "    First, carefully review the following description of the concept:\n",
    "\n",
    "    <concept_description>\n",
    "    {concept_description}\n",
    "    </concept_description>\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    1. Read through each passage carefully, thinking about the concept and different ways it is used in documents.\n",
    "    2. Identify any mentions of the concept, including direct references and related terms.\n",
    "    3. Surround each identified mention with <concept> tags.\n",
    "    4. If a passage contains multiple instances, each one should be tagged separately.\n",
    "    5. If a passage does not contain any instances, it should be reproduced exactly as given, without any additional tags.\n",
    "    6. If an entire passage refers to the concept without specific mentions, the entire passage should be wrapped in a <concept> tag.\n",
    "    7. The input text must be reproduced exactly, down to the last character, only adding concept tags.\n",
    "    8. Double check that you have tagged all financial flows and that every tagged part is describing an actual financial flow, including the monetary value, if any is given.\n",
    "    \"\"\",\n",
    "    ),\n",
    "    \"examples_added\": LLMClassifier(\n",
    "        new_concept,\n",
    "        model_name=\"gpt-4o\",\n",
    "        system_prompt_template=\"\"\"\n",
    "    You are a specialist analyst, tasked with identifying mentions of \n",
    "    concepts in policy documents. You will mark up references to concepts with \n",
    "    XML tags.\n",
    "\n",
    "    First, carefully review the following description of the concept:\n",
    "\n",
    "    <concept_description>\n",
    "    {concept_description}\n",
    "    </concept_description>\n",
    "\n",
    "    then read the annotation guidelines and examples\n",
    "    <guidelines>\n",
    "    Rules\n",
    "    - Include when the text describes money or financial assets moving (e.g. payments, loans, investments, disbursements, repayments).\n",
    "    - Exclude when the text only states amounts held, owed, or valued at a point in time (these are stocks, not flows).\n",
    "    - Include both one-off transactions and ongoing streams (e.g. monthly payments, yearly disbursements).\n",
    "    - Exclude metaphorical/non-financial uses of “flow” (e.g. “flow of information”).\n",
    "\n",
    "    Positive Examples (label as “financial flow”)\n",
    "    “The government confirmed $10 million to level up towns.”\n",
    "    “Foreign direct investment inflows totalling €2 billion in 2023.”\n",
    "    “The charity disbursed £500,000 to local projects.”\n",
    "    “Additional funds shall be made available to provide grants to States”\n",
    "\n",
    "    Negative Examples (do NOT label as “financial flow”)\n",
    "    “The company’s assets are worth $5 billion.” (stock/valuation)\n",
    "    “GDP increased by $2 billion” (not flowing between parties)\n",
    "    “Information flows quickly in digital markets.” (not financial)\n",
    "\n",
    "    Borderline / Ambiguous Cases\n",
    "    “Interest accrued on deposits.” → Only label if text explicitly describes interest being paid or transferred.\n",
    "    “The value of foreign reserves increased by $10 billion.” → Could be due to flows or valuation changes; only label if the movement (purchase/sale/transfer) is clear from the text or context.\n",
    "    </guidelines>\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    1. Read through each passage carefully, thinking about the concept and different ways it is used in documents.\n",
    "    2. Identify any mentions of the concept, including direct references and related terms.\n",
    "    3. Surround each identified mention with <concept> tags.\n",
    "    4. If a passage contains multiple instances, each one should be tagged separately.\n",
    "    5. If a passage does not contain any instances, it should be reproduced exactly as given, without any additional tags.\n",
    "    6. If an entire passage refers to the concept without specific mentions, the entire passage should be wrapped in a <concept> tag.\n",
    "    7. The input text must be reproduced exactly, down to the last character, only adding concept tags.\n",
    "    8. Double check that you have tagged all financial flows and that every tagged part is describing an actual financial flow, including the monetary value, if any is given.\n",
    "    \"\"\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write some helper functions to make my life easier\n",
    "# and then use that to set up some experiments\n",
    "# First, let's create a function to get tagged passages in a readable format\n",
    "def get_tagged_passages(classifier, texts):\n",
    "    results = []\n",
    "    # Use predict_batch instead of individual predict calls\n",
    "    try:\n",
    "        all_spans = classifier.predict_batch(texts)\n",
    "\n",
    "        # Match spans with their original texts\n",
    "        for text, spans in notebook_tqdm.tqdm(\n",
    "            zip(texts, all_spans), total=len(texts), desc=\"Processing tagged passages\"\n",
    "        ):\n",
    "            if spans:  # Only include texts that got tagged\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"text\": text,\n",
    "                        \"tagged_spans\": [span.labelled_text for span in spans],\n",
    "                    }\n",
    "                )\n",
    "        return results\n",
    "    except:\n",
    "        print(f\"ERROR for classifier {classifier}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def compare_setups(setups, sample_texts):\n",
    "    \"\"\"\n",
    "    setups: dict of {setup_name: classifier}\n",
    "    sample_texts: list of texts to test\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for setup_name, classifier in notebook_tqdm.tqdm(\n",
    "        setups.items(), desc=\"Comparing setups\"\n",
    "    ):\n",
    "        tagged = get_tagged_passages(classifier, sample_texts)\n",
    "        results[setup_name] = {\"n_tagged\": len(tagged), \"tagged_passages\": tagged}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_comparison_table(results):\n",
    "    \"\"\"\n",
    "    Create a comparison table that includes both quantitative and qualitative data.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store the data\n",
    "    comparison_data = {\n",
    "        \"text\": sample_texts,  # Original texts\n",
    "    }\n",
    "\n",
    "    # Add columns for each setup\n",
    "    for setup_name, result in results.items():\n",
    "        # Create a dictionary mapping texts to their tagged spans\n",
    "        text_to_spans = {\n",
    "            p[\"text\"]: p[\"tagged_spans\"] for p in result[\"tagged_passages\"]\n",
    "        }\n",
    "\n",
    "        # Add both the binary tag and the actual spans\n",
    "        comparison_data[f\"{setup_name}_tagged\"] = [\n",
    "            1 if text in text_to_spans else 0 for text in sample_texts\n",
    "        ]\n",
    "        comparison_data[f\"{setup_name}_spans\"] = [\n",
    "            text_to_spans.get(text, []) for text in sample_texts\n",
    "        ]\n",
    "\n",
    "    # Create DataFrame only once at the end\n",
    "    return pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparison\n",
    "sample_texts = (\n",
    "    df[df[\"language\"] == \"en\"].sample(n=150, random_state=42)[\"text\"].tolist()\n",
    ")\n",
    "sample_texts = [str(t) for t in sample_texts]\n",
    "results = compare_setups(setups, sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the comparison table\n",
    "comparison_table = create_comparison_table(results)\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"Total passages tagged by each setup:\")\n",
    "print(\n",
    "    comparison_table[\n",
    "        [col for col in comparison_table.columns if col.endswith(\"_tagged\")]\n",
    "    ].sum()\n",
    ")\n",
    "\n",
    "# Show passages where setups disagree\n",
    "print(\"\\nPassages where setups disagree:\")\n",
    "disagreements = comparison_table[\n",
    "    comparison_table[\n",
    "        [col for col in comparison_table.columns if col.endswith(\"_tagged\")]\n",
    "    ].nunique(axis=1)\n",
    "    > 1\n",
    "]\n",
    "\n",
    "# For each disagreeing passage, show the different spans identified\n",
    "for _, row in disagreements.iterrows():\n",
    "    print(\"\\nOriginal text:\", row[\"text\"])\n",
    "    for setup in setups.keys():\n",
    "        if row[f\"{setup}_tagged\"]:\n",
    "            print(f\"{setup} identified:\", row[f\"{setup}_spans\"])\n",
    "        else:\n",
    "            print(f\"{setup} identified: [NO TAGS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table[\n",
    "    (comparison_table[\"best_effort_tagged\"] + comparison_table[\"examples_added_tagged\"])\n",
    "    > 0\n",
    "].to_csv(\"examples_add.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT 4 mini only gives errors. 4o-mini gives fairly consistent outputs but it's better at finding \"finance vibes\" than \"finance flows\". 4o is much more strictly adhering to the description and relies less on the examples. Markedly better overall. \n",
    "- general template is pretty general, but sometimes this means it picks up on useful grey areas\n",
    "- unsure why the small change in the template to be non-climate specific led to such different results.\n",
    "- \"tagging every instance\" seems to be interpreted differently between models & might be responsible for some of the over-enthusiasm. \n",
    "- double-checking step at the end might be useful? Particularly because it encourages to include the actual flow & currencies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From small examples to medium size\n",
    "We got this working for a few hundred passages at a time. Now that we have an idea of what works, let's try to predict (in batches) for a much larger dataframe so we can do a more formal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subset\n",
    "medium_df = df.sample(n=3000, random_state=420)\n",
    "medium_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some final tweaks to create our true best effort classifier\n",
    "# Leaving out the alt labels as they seem to bias it to short snippets\n",
    "# Add in examples, but a slightly more varied bunch than earlier\n",
    "# And sharpen the language to be more in line with CPI's understanding of a finance flow\n",
    "\n",
    "guidelines = \"\"\"\n",
    "\n",
    "Rules\n",
    "    - Include when the text describes money or financial assets moving (e.g. payments, loans, investments, disbursements, repayments).\n",
    "    - Include both one-off transactions and ongoing streams (e.g. monthly payments, yearly disbursements).\n",
    "    - Exclude when the text only states amounts held, owed, or valued at a point in time (these are stocks, not flows).\n",
    "    - Exclude metaphorical/non-financial uses of “flow” (e.g. “flow of information”).\n",
    "\n",
    "    Positive Examples (label as “financial flow”)\n",
    "    “The government confirmed $10 million to level up towns.”\n",
    "    “Foreign direct investment inflows totalling €2 billion in 2023.”\n",
    "    “The charity disbursed £500,000 to local projects.”\n",
    "    “Additional funds shall be made available to provide grants to States”\n",
    "    \"GCF funding for capacity building at similar levels to last year\"\n",
    "\n",
    "    Negative Examples (do NOT label as “financial flow”)\n",
    "    “The company’s assets are worth $5 billion.” (stock/valuation)\n",
    "    “GDP increased by $2 billion” (not flowing between parties)\n",
    "    “Information flows quickly in digital markets.” (not financial)\n",
    "\"\"\"\n",
    "\n",
    "final_concept = Concept(\n",
    "    preferred_label=\"Finance flow\",\n",
    "    wikibase_id=\"Q1829\",  # Placeholder - needed for Argilla\n",
    "    # alternative_labels = ['flow of finance', 'financial transfer', 'establish fund', 'monetary flow', 'investing USD', 'loan to', 'subsidy totalling', 'payment to', 'paying taxes', 'spending euros on', 'raising money for', 'add to the financial budget'],\n",
    "    description=\"Either money or something of monetary value is transferred between two or more organisations, people or countries.\",\n",
    "    definition=f\"\"\"\n",
    "    A finance flow is an economic flow that reflects the creation, transformation, exchange, transfer, or extinction of economic value and involves changes in ownership of goods and/or financial assets, the provision of services, or the provision of labor and capital.\n",
    "    ideally, a financial flow describes four elements: \n",
    "    1. the source (who is sending the financial asset, such as a bank or organisation); \n",
    "    2. the financial instrument or mechanism (how it is being sent, such as a grant, loan or a subsidy);\n",
    "    3. the use or destination (the purpose for which the asset will be used, which is often expressed as the recipient organisation or their sectoral categorisation); and \n",
    "    4. the value (which can, but does not need to be, expressed in monetary terms directly). \n",
    "    It is acceptable if not all of these elements are present, but where they are, all of these elements should be considered part of the same financial flow.\n",
    "\n",
    "    Guidelines for annotation are:\n",
    "    {guidelines}\n",
    "\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "classifier = LLMClassifier(\n",
    "    final_concept,\n",
    "    model_name=\"gpt-4o\",\n",
    "    system_prompt_template=\"\"\"\n",
    "    You are a specialist analyst, tasked with identifying mentions of concepts in policy documents. \n",
    "    These documents are mostly drawn from a climate and development context.\n",
    "    You will mark up references to concepts with XML tags.\n",
    "\n",
    "    First, carefully review the following description of the concept:\n",
    "\n",
    "    <concept_description>\n",
    "    {concept_description}\n",
    "    </concept_description>\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    1. Read through each passage carefully, thinking about the concept and different ways it can be used in documents.\n",
    "    2. Identify any mentions of the concept, including references that are not included as an example, but which match the definition and guidelines.\n",
    "    3. Surround each identified mention with <concept> tags.\n",
    "    4. If a passage contains multiple instances, each one should be tagged separately.\n",
    "    5. If a passage does not contain any instances, it should be reproduced exactly as given, without any additional tags.\n",
    "    6. If an entire passage refers to the concept without specific mentions, the entire passage should be wrapped in a <concept> tag.\n",
    "    7. The input text must be reproduced exactly, down to the last character, only adding concept tags.\n",
    "    8. Double check that you have tagged all financial flows and that every tagged part is describing an actual financial flow, including the source, destination, instrument and value, if any is given.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lil safety mechanism to stop me burning through our credits\n",
    "predict_the_medium_sample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import date\n",
    "\n",
    "from knowledge_graph.labelled_passage import LabelledPassage\n",
    "\n",
    "# Predict in batches of 100 at a time\n",
    "# Note that we are planning to upload this to argilla/vibe checker so need metadata\n",
    "if predict_the_medium_sample:\n",
    "    batch_labelled_passages = []\n",
    "    for batch in range(0, len(medium_df), 100):\n",
    "        batch_dataset = medium_df.iloc[batch : batch + 100]\n",
    "        batch_texts = batch_dataset[\"text\"].tolist()\n",
    "        batch_spans = classifier.predict_batch(batch_texts)\n",
    "        batch_labelled_passages.extend(\n",
    "            LabelledPassage(\n",
    "                text=text, spans=spans, metadata=batch_dataset.iloc[i].to_dict()\n",
    "            )\n",
    "            for i, (text, spans) in enumerate(zip(batch_texts, batch_spans))\n",
    "        )\n",
    "    # To stop me automatically running it again\n",
    "    predict_the_medium_sample = False\n",
    "    date = date.today().isoformat()\n",
    "\n",
    "    with open(f\"../data/processed/{date}_llm_finance_predictions.pickle\", \"wb\") as file:\n",
    "        pickle.dump(batch_labelled_passages, file)\n",
    "else:\n",
    "    date = date.today().isoformat()\n",
    "\n",
    "    with open(\n",
    "        \"../data/processed/2025-09-18_llm_finance_predictions.pickle\", \"rb\"\n",
    "    ) as file:\n",
    "        batch_labelled_passages = pickle.load(file)\n",
    "\n",
    "\n",
    "batch_labelled_passages[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to vibe checker and Argilla\n",
    "Both of these take lists of labelled passages with the LabelledPassage object.\n",
    "Every prediction has already been made into such an object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to show how it works\n",
    "from knowledge_graph.labelled_passage import LabelledPassage\n",
    "\n",
    "sentence = \"This sentence is about climate finance flows\"\n",
    "\n",
    "classifier = LLMClassifier(new_concept, model_name=\"gpt-4o\")\n",
    "\n",
    "spans = classifier.predict(sentence)\n",
    "labelled_passage = LabelledPassage(text=sentence, spans=spans)\n",
    "labelled_passage.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the set to upload: positive ones plus some share of negatives\n",
    "# First, let's just see how many positive ones we have\n",
    "n = 0\n",
    "for passage in batch_labelled_passages:\n",
    "    prediction = passage.spans\n",
    "    if len(prediction) > 0:\n",
    "        n += 1\n",
    "print(f\"{n} passages with at least one tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take 350 of those and then add in an additional 150 passages without any matches.\n",
    "# 1) 350 with matches, 2) +150 without matches\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "positives = [lp for lp in batch_labelled_passages if len(lp.spans) > 0]\n",
    "negatives = [lp for lp in batch_labelled_passages if len(lp.spans) == 0]\n",
    "\n",
    "pos_sample = random.sample(positives, min(350, len(positives)))\n",
    "neg_sample = random.sample(negatives, min(200, len(negatives)))\n",
    "\n",
    "selected_passages = pos_sample + neg_sample\n",
    "random.shuffle(selected_passages)\n",
    "\n",
    "print(f\"Positives sample: {len(pos_sample)} out of {len(positives)}\")\n",
    "print(f\"Negatives sample: {len(neg_sample)} out of {len(negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def fix_pub_ts(md):\n",
    "    for k in (\"publication_ts\", \"document_metadata.publication_ts\"):\n",
    "        if k in md:\n",
    "            v = md[k]\n",
    "            try:\n",
    "                v = pd.to_datetime(v, errors=\"raise\")\n",
    "            except:\n",
    "                print(v)\n",
    "                v = None\n",
    "                pass\n",
    "            if isinstance(v, (pd.Timestamp, datetime)):\n",
    "                md[k] = v.isoformat()\n",
    "            elif isinstance(v, (float, np.floating)) and np.isnan(v):\n",
    "                md[k] = None\n",
    "            else:\n",
    "                md[k] = str(v)\n",
    "\n",
    "\n",
    "for lp in selected_passages:\n",
    "    if isinstance(lp.metadata, dict):\n",
    "        fix_pub_ts(lp.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../data/processed/finance_flow_labelled_passages_add_id.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join([passage.model_dump_json() for passage in selected_passages]))\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save as a csv -- but first clean up the file a little\n",
    "with open(out_path, \"r\") as f:\n",
    "    df_passage = pd.read_json(f, lines=True)\n",
    "df_passage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(df_passage[\"metadata\"].to_list())\n",
    "df_passage.drop(columns=\"metadata\")\n",
    "df_passage = pd.concat([df_passage, df_meta], axis=1)\n",
    "df_passage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in the spans is a bit more of a pain as some of them are None\n",
    "spans_list = []\n",
    "for row in df_passage[\"spans\"]:\n",
    "    tl = []\n",
    "    for match in row:\n",
    "        if match != None:\n",
    "            tl.append(match[\"labelled_text\"])\n",
    "    spans_list.append(tl)\n",
    "\n",
    "spans_df = pd.DataFrame(spans_list)\n",
    "df_passage = pd.concat([spans_df, df_passage], axis=1)\n",
    "df_passage.to_csv(\n",
    "    f\"../data/out/{date}_finance_flows.csv\", index=False, encoding=\"utf-8\"\n",
    ")\n",
    "df_passage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harrison will do: JSONL of labelled passages => S3 bucket => vibe checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_passages = selected_passages.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata seems like it's causing problems in the upload\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sanitize(v):\n",
    "    # pandas NA\n",
    "    if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "        return None\n",
    "    if isinstance(v, (np.floating,)):\n",
    "        return None if (np.isnan(v) or np.isinf(v)) else float(v)\n",
    "    if pd.isna(v):  # catches pd.NaT, pd.NA\n",
    "        return None\n",
    "    if isinstance(v, (np.integer,)):\n",
    "        return int(v)\n",
    "    if hasattr(v, \"tolist\"):  # numpy arrays\n",
    "        return sanitize(v.tolist())\n",
    "    if isinstance(v, dict):\n",
    "        return {str(k): sanitize(val) for k, val in v.items()}\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        return [sanitize(x) for x in v]\n",
    "    return v\n",
    "\n",
    "\n",
    "for lp in modified_passages:\n",
    "    if isinstance(lp.metadata, dict):\n",
    "        md = {k: sanitize(v) for k, v in lp.metadata.items() if k != \"text\"}\n",
    "        lp.metadata = md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mapper = {\n",
    "    \"text\": \"text\",\n",
    "    \"text_block_id\": \"text_block-text_block_id\",\n",
    "    \"language\": \"text_block-language\",\n",
    "    \"type\": \"text_block-type\",\n",
    "    \"type_confidence\": \"text_block-type_confidence\",\n",
    "    \"page_number\": \"text_block-page_number\",\n",
    "    \"coords\": \"text_block-coords\",\n",
    "    \"document_id\": \"document_id\",\n",
    "    \"document_name\": \"document_name\",\n",
    "    \"document_source_url\": \"document_source_url\",\n",
    "    \"document_content_type\": \"document_content_type\",\n",
    "    \"document_md5_sum\": \"document_md5_sum\",\n",
    "    \"languages\": \"languages\",\n",
    "    \"translated\": \"translated\",\n",
    "    \"has_valid_text\": \"has_valid_text\",\n",
    "    \"pipeline_metadata\": \"pipeline_metadata\",\n",
    "    \"name\": \"document_metadata-name\",\n",
    "    \"document_title\": \"document_metadata-document_title\",\n",
    "    \"description\": \"document_metadata-description\",\n",
    "    \"import_id\": \"document_metadata-import_id\",\n",
    "    \"slug\": \"document_metadata-slug\",\n",
    "    \"family_import_id\": \"document_metadata-family_import_id\",\n",
    "    \"family_slug\": \"document_metadata-family_slug\",\n",
    "    \"publication_ts\": \"document_metadata-publication_ts\",\n",
    "    \"date\": \"document_metadata-date\",\n",
    "    \"source_url\": \"document_metadata-source_url\",\n",
    "    \"download_url\": \"document_metadata-download_url\",\n",
    "    \"corpus_import_id\": \"document_metadata-corpus_import_id\",\n",
    "    \"corpus_type_name\": \"document_metadata-corpus_type_name\",\n",
    "    \"collection_title\": \"document_metadata-collection_title\",\n",
    "    \"collection_summary\": \"document_metadata-collection_summary\",\n",
    "    # \"document_metadata-type\",\n",
    "    \"source\": \"document_metadata-source\",\n",
    "    \"category\": \"document_metadata-category\",\n",
    "    \"geography\": \"document_metadata-geography\",\n",
    "    \"geographies\": \"document_metadata-geographies\",\n",
    "    # \"document_metadata-languages\",\n",
    "    \"metadata\": \"document_metadata-metadata\",\n",
    "    \"document_description\": \"document_description\",\n",
    "    \"document_cdn_object\": \"document_cdn_object\",\n",
    "    \"document_slug\": \"document_slug\",\n",
    "    \"md5sum\": \"pdf_data-md5sum\",\n",
    "    \"dimensions\": \"pdf_data_page_metadata-dimensions\",\n",
    "    # \"pdf_data_page_metadata-page_number\",\n",
    "    \"detected_title\": \"_html_data-detected_title\",\n",
    "    \"detected_date\": \"_html_data-detected_date\",\n",
    "    # \"_html_data-has_valid_text\",\n",
    "    \"parser_metadata\": \"pipeline_metadata-parser_metadata\",\n",
    "    \"index\": \"text_block-index\",\n",
    "    \"world_bank_region\": \"world_bank_region\",\n",
    "}\n",
    "\n",
    "for passage in modified_passages:\n",
    "    # Create new metadata dict with only mapped fields\n",
    "    new_metadata = {}\n",
    "\n",
    "    for old_key, value in passage.metadata.items():\n",
    "        if old_key in field_mapper:\n",
    "            new_key = field_mapper[old_key]\n",
    "            new_metadata[new_key] = value\n",
    "        # Any keys not in field_mapper will be dropped (superfluous keys)\n",
    "\n",
    "    # Replace the old metadata with cleaned metadata\n",
    "    passage.metadata = new_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_passages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argilla: look in the github; has an example\n",
    "\n",
    "from knowledge_graph.labelling import ArgillaSession\n",
    "\n",
    "session = ArgillaSession()  # .env credentials!\n",
    "\n",
    "# If we need to create a new workspace:\n",
    "# workspace_to_create = Workspace(name=\"finance-experiments\")\n",
    "# workspace = workspace_to_create.create()\n",
    "\n",
    "# If it already exists:\n",
    "workspace = session.client.workspaces(name=\"finance-experiments\")\n",
    "\n",
    "argilla_data = session.labelled_passages_to_dataset(\n",
    "    labelled_passages=modified_passages, concept=final_concept, workspace=workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = session.client.users(\"sion\")\n",
    "workspace = session.client.workspaces(\"finance-experiments\")\n",
    "\n",
    "added_user = user.add_to_workspace(workspace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
